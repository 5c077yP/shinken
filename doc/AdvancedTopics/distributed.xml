<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML V4.4//EN"
	  "http://www.oasis-open.org/docbook/xml/4.4/docbookx.dtd"
	  [
	  <!ENTITY % all.entities SYSTEM "../all-entities.ent">
	  %all.entities;
	  ]
	  >
<chapter id="advancedtopics-distributed" xreflabel="Distributed Monitoring">
  <title>Distributed Monitoring</title>
  <section>
    <title>Introduction</title>
    <para>Shinken can be configured to support distributed monitoring of network services 
      and resources. It's even it's main goal in front of Nagios way of doing it : it's natural 
      in the Shinken's architecture, where is it more a MacGyver way in Nagios.</para>
  </section>
  <section>
    <title>Goals</title>
    <para>The goal in the distributed monitoring environment that I will describe is
      to offload the overhead (CPU usage, etc.) of performing service checks from a "central"
      server onto one or more "distributed" servers. Most small to medium sized shops will 
      not have a real need for setting up such an environment. However, when you want to
      start monitoring thousands of hosts (and several times that many services) 
      using Shinken, this becomes quite important.</para>
  </section>
  <section>
    <title>The global architecture</title>
    <para>Shinken's architecture has been designed according to the Unix Way : one tool, one task. Right now the Nagios daemon does nearly everything: loads configuration, schedule and launch checks, and raise notifications. Henceforth shinken has an architecture where each part is isolated and connected to the others via a communication channel which makes building distributed monitoring architecture quite easy.
      The major innovation of Shinken over nagios is to split the different roles into separated daemons. They are :
    </para>
    <itemizedlist>
      <listitem>
	<para>Arbiter : it reads the configuration, cuts it into parts (N schedulers = N parts), and then send them to all others elements. It manages the high availability part : if an element dies, it re-routes the configuration managed by this falling element to a spare one. Its other role is to receive input from users (like external commands of nagios.cmd) and send them to other elements. There can be only one active arbiter in the architecture.</para>
      </listitem>	  
      <listitem>
	<para>Schedulers : they are in charge of the scheduling checks, the analysis of results and follow up actions (like if a service is down, ask for a host check). They do not launch checks or notifications. They keep a queue of pending checks and notifications for other elements of the architecture (like pollers or reactionners). There can be many schedulers.</para>
      </listitem>
      <listitem>
	<para>Pollers : They are in charge of launching plugins as requested by schedulers. When the check is finished they return the result to the schedulers. There can be many pollers.</para>
      </listitem>
      <listitem> 
	<para>Reactionners : They are in charge of notifications and launching event_handlers. They are not managed by pollers because it is more easy to have only one place to send notifications (and another one for spare) for example to have less SMTP authorisations or RSS feeds to read (only one for all hosts/services). There can be numerous reactionners if the administrator desires so.
	</para>
      </listitem>
      <listitem>
	<para>Broker : Its role is to get data from schedulers (like status) and manage them (like storing them in database). The management is done by modules. Different modules exists : export into ndo database (MySQL and Oracle backend), export to merlin database (MySQL), service-perfdata export and a couchdb export.
	</para>
      </listitem>
    </itemizedlist>
    <para> This architecture is fully flexible and scalable: the only daemons that ask for performances are pollers and schedulers and the administrator can add as much as he wants. A picture is worth a thousand words:
      TODO : picture
    </para>
    <mediaobject>
      <imageobject>
	<imagedata fileref="images/shinken-architecture.png" format="PNG"/>
      </imageobject>
    </mediaobject>

  </section>
  <section>
    <title>The smart and automatic load balancing</title>
    <para>
      Shinken is able to cut the user configuration into part and dispatch it to schedulers.
      The load balancing is done automaticaly : the administrator do not need to remember which 
      host is link with another one to create packs, Shinken do it for him.
    </para>
    <para>
      The dispatch is a host based one : that mean all services of a host will be in the same scheduler than this host.
      The major avantage of Shinken is the ability to create independants configurations : a element of a configuration
      will not have to call a element of another pack. That mean the administrator do nto need to know all realations 
      betweens elements like parents, hostdependancies or services dependancies : Shinken is able to look at theses 
      realtions and put theses related elements into the same packs.
    </para>
    <para>
      This action is done in two parts:
    </para>
    <itemizedlist>
      <listitem>
	<para>
	  create independant packs of elements
	</para>
      </listitem>
      <listitem>
	<para>
	  paste packs to create N configurations for the N schedulers
	</para>
      </listitem>
    </itemizedlist>
    <section>
      <title> Creating independants packs</title>
      <para>
	The cutting action is done by looking at two elements : hosts and services. Services are linked with their how host so they will be in the same pack. Others relations are taken into account :
      </para>
      <itemizedlist>
	<listitem>
	  <para>parent relationshinp for hosts (like a distant server and it's router)</para>
	</listitem>
	<listitem>
	  <para>hostdependancies</para>
	</listitem>
	<listitem>
	  <para>servicesdependancies</para>
	</listitem>
      </itemizedlist>
      <para>
	Shinken look at all theses relations and create a graph with it. A graph is a relation pack. This can be illustrated by the following picture :
      </para>
      <mediaobject>
	<imageobject>
	  <imagedata fileref="images/pack-creation.png" format="PNG"/>
	</imageobject>
      </mediaobject>

      <para>In this example, we will have two packs:
      </para>
      <itemizedlist>
	<listitem>
	  <para>pack 1 : Host-1 to host-5 and all theirs services</para>
	</listitem>
	<listitem>
	  <para>pack 2 : Host-6 to Host-8 and all theirs services</para>
	</listitem>
      </itemizedlist>
    </section>
    <section>
      <title>The packs agreagations into scheduler configurations</title>
      <para>      
	When all relation packs are created, the Arbiter agregate them info N configurations if the administrator have defined N active schedulers (no spare). Packs are agregate into configurations (it's like "Big packs"). The dispatch look at the weight property of schedulers : the higer weight the scheduler have, the more packs it will have.
	This can be show in the folowing picture :
      </para>
      <mediaobject>
	<imageobject>
	  <imagedata fileref="images/pack-agregation.png" format="PNG"/>
	</imageobject>
      </mediaobject>

    </section>
    <section>
      <title>The configurations sending to satellites</title>
      <para>      
	When all configurations created, the Arbiter send them to the N active schedulers. It also create configurations for satellites (pollers, reactionners and brokers) with links to schedulers so they know where to get jobs to do.  After sending the configurations, the Arbiter begin to watch orders from the users and see if satellites are still alive.
      </para>
      
    </section>
  </section>
  <section>
    <title> The high availability </title>
    <para>
      The shinken architecture is a high availability one. Before looking at how this works,to take a look at how the load balancing works if it's now already done.
    </para>
    <section>
      <title>When a node dies</title>
      
      <para>
	Nobody is perfect. A server can crash, an application too. That is why administrators have spares: they can take configurations of failing elements and reassign them. For the moment the only daemon to not have a spare is the Arbiter, but it wil be add in the future.
	
	The Arbiter regulary checks if everyone is available. If a scheduler or another satellite is dead it sends its conf to a spare  nodedefined by the administrator. All satellites are informed by this change so they can get they jobs from the new element and do not try to reach the dead one.
	
	If the lost node was caused by a network interruption and it comes back up the Arbiter will notice and ask the old system to drop its configuration.
	
	This can be explained by the following picture :
      </para>
      <mediaobject>
	<imageobject>
	  <imagedata fileref="images/pack-creation.png" format="PNG"/>
	</imageobject>
      </mediaobject>
    </section>
  </section>
  <section>
    <title>External commands dispatching</title>
    <para>
      
      The administrator needs to send orders to the schedulers (like a new status for passive checks). In the Shinken way of thinking, the users only need to send orders to one daemon that will then dispatch them to all others. In Nagios the administrator needs to know where the hosts or services is to send the order to the right node. In Shinken the administrator just sends the order to the Arbiter, that's all.

      Externals commands can be divided into two types :
    </para>
    <itemizedlist>	  
      <listitem> 
	<para>commands that are globals to all schedulers</para>
      </listitem>
      <listitem>
	<para>commands that are specific to one element (host/service).</para>
      </listitem>
    </itemizedlist>
    <para>
      For each commands, Shinken knows if it is global or not. If global, it just sends orders to all schedulers. For specific ones instead it searches which scheduler manages the element referred by the command (host/service) and sends the order to this scheduler. When the order is received by schedulers they just need to apply them.
    </para>
  </section>

  <section>
    <title>Different type of Pollers : poller_tag</title>
    <para>
      Current Shinken's architecture is useful for someone that use the same type of poller for checks. 
      But it can be useful to have different types of pollers, like GNU/Linux ones and Windows ones. 
      We already saw that all pollers talks to all schedulers. In fact, pollers can be "taggued" so they
      will only got some checks.
    </para>
    <para>This is useful when user need to have hosts in the same scheduler (like with dependancies) but
      need some hosts or services to be checks by specific pollers (see uses cases below).
    </para>
    <para>
      Theses checks can in fact be taggued in 3 levels :
      <itemizedlist>
	<listitem>
	  <para>Host</para>
	</listitem>
	<listitem>
          <para>Service</para>
        </listitem>
	<listitem>
	  <para>Command</para>
	</listitem>
      </itemizedlist>
    </para>
    <para>
      The parameter to tag a command, host ro service is "poller_tag". If a check use a "taggued" or "untaggued" command 
      in a untaggued host/service, it takes the poller_tag of this host/service. In a "untagued" host/service, it's the
      command tag that is taken into account.
    </para>
    <para>
      The pollers can be taggued with multiples poller_tags. If they are taggued, they will only take checks that are taggued, not
      the untaggued ones.
    </para>

    <section>
      <title>Use cases</title>
      <para>
	This capability is useful in two cases:
	<itemizedlist>
        <listitem>
          <para>GNU/Linux and Windows pollers</para>
        </listitem>
        <listitem>
          <para>DMZ</para>
        </listitem>
      </itemizedlist>
      </para>
      <para>
	In the first case, it can be useful to have a windows box in a domain with poller daemon running under a domain account.
	If this poller launch WMI queries, user can have a easy Windows monitoring.
      </para>
      <para>
        In the second case is a classic one : when you have a DMZ network, you need to have a dedicated poller that 
	is in the DMZ, and return results to a scheduler in LAN. With this, you can still have dependancies between
	DMZ hosts and LAN hosts, and still be sure checks are done in a DMZ only poller.
      </para>
    </section>
  </section>

  <section>
    <title>Advanced architectures : Realms</title>
    <para>
      Shinken's architecture allows the administrator to have a unique point of administration with numerous schedulers, pollers, reactionners and brokers. Hosts are dispatched with their own services to schedulers and the satellites (pollers/reactionners/brokers) get jobs from them. Everyone is happy.
    </para>
    <para>
      Or almost everyone. Think about an administrator who has a distributed architecture around the world. With the current
      Shinken architecture the administrator can put a couple scheduler/poller in Europe and another in Asia, but he cannot
      "tag" hosts in Asia to be checked by the asian scheduler . And try ingto check an asian server with an european
      scheduler can be... random. The hosts are dispatched to all schedulers and satellites so the administrator cannot
      be sure than asian hosts will be checked by the asian monitoring servers.
    </para>
    <para>
      In the current Architecture Shinken is usefull for load balancing with high availability, but for a 
      unique site. Site managment must be added.
    </para>
    <para>
      We will use a generic term for this site managment because maybe this partitioning is not a geographic 
      one, but an organisational one. We will then use the name  realm.
    </para>
    <section>
      <title>Realms in few words</title>
      <para>      
	A realm is a pool of resources (scheduler, poller, reactionner and broker) that hosts or hostgroups
	can be attached to. A host or hostgroup can be attached to only one realm. All "dependancies" or
	parents of this hosts must be in the same realm. A realm can be tagged "default"' and 
	realm untagged hosts will be put into it.

	In a realm, pollers, reactionners and brokers will only get jobs from scehdulers of the same realm.
      </para>
    </section>
    <section>
      <title>Realms are not poller_tags!</title>
      <para>
	Beware to understand when to use realms and when to use poller_tags. Realms are use to divise schedulers, 
	poller_tags for divise pollers. If some cases of poller_tag can be done with realm, think taht you must see if poller_tag is
	"enough" for you before use realms. In realms, hosts between schedulers do not communicate. If you just need a poller in a DMZ
	network, use poller_tag. If you need a scheduler/poller in a customer LAN, use realms.
      </para>
    </section>
    <section>

      <title>Sub realms</title>
      <para>
	In fact my last sentence is not exact. A realm can contain another realm. It does not change anything for schedulers: they only got host of their realm not the ones of the sub realms. The realm tree is useful for satellites like reactionners or brokers : they can get jobs from the schedulers of their realm, but also from schedulers of sub realms. Poller can also get jobs from sub realms, but it's less useful so it's disabled by default.

	Warning: having more than one broker in a scheduler is not a good idea. The jobs for brokers can be taken by only one broker.
	For the Arbiter it doe not change a thing: there is still only one Arbiter and one configuration whatever realms you have.
      </para>
    </section>
    <section>
      <title>Example of realm usage</title>
      <para>      
	Let's take a look at two distributed envrionnements. In the first case the administrator wants totally distinct daemons. In the second one he just wants the schedulers/pollers to be distincts, but still have one place to send notifications (reactionners) and one place for database export (broker).
      </para>
      <para>
	Distincts realms :
      </para>
      <mediaobject>
	<imageobject>
	  <imagedata fileref="images/shinken-architecture-isolated-realms.png" format="PNG"/>
	</imageobject>
      </mediaobject>
      <para>
	More comon usage, the global realm with reactionner/broker, and sub realms with schedulers/pollers :
      </para>
      <mediaobject>
	<imageobject>
	  <imagedata fileref="images/shinken-architecture-global-realm.png" format="PNG"/>
	</imageobject>
      </mediaobject>
      <para>
	Satellites can be used for their realm or sub realms too. It's just a parameter in the configuration of the element.
      </para>
    </section>
  </section>
</chapter>

